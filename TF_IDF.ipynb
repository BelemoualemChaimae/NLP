{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPBVzpi0KgNu2Yqp8h1QaFn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BelemoualemChaimae/NLP/blob/main/TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwcA161h_J4F"
      },
      "source": [
        "from pprint import pprint "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yerVjhxYBQot",
        "outputId": "27aee36a-e06f-4189-a4b0-106d63affecb"
      },
      "source": [
        "pip install gensim "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfpXgl27BkQd",
        "outputId": "a1f48cd1-3024-4152-eb08-4290518c85a3"
      },
      "source": [
        "from pprint import pprint \n",
        "from gensim import corpora , models , similarities\n",
        "\n",
        "documents=[\"new york times\",\n",
        "           \"new york post\",\n",
        "           \"loss angless times\"]\n",
        "#Preprocessing Here we only do Tokenization \n",
        "tokenized_documents=[[token for token in document.lower().split()] for document in documents]\n",
        "pprint(tokenized_documents)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['new', 'york', 'times'],\n",
            " ['new', 'york', 'post'],\n",
            " ['loss', 'angless', 'times']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxk7oWHgC9tj",
        "outputId": "ce272d35-0288-40cf-f5b2-377e9c6788bf"
      },
      "source": [
        "# Creating the Dictionary (oka --> ID --> Word (id2word) mapping)\n",
        "dictionary=corpora.Dictionary(tokenized_documents)\n",
        "print(dictionary)\n",
        "print(\"\\n\")\n",
        "print(dictionary.num_docs)\n",
        "\n",
        "#save dictionary as text for corpus inspection \n",
        "#dictionary.save_as_text()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dictionary(6 unique tokens: ['new', 'times', 'york', 'post', 'angless']...)\n",
            "\n",
            "\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wtZ-p-wGnSp",
        "outputId": "c1e81efa-03ae-40b7-b3f7-4cc950748436"
      },
      "source": [
        "# To see the mapping between words and their id \n",
        "print(dictionary.token2id)\n",
        "print(\"\\n\",dictionary[0],dictionary[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'new': 0, 'times': 1, 'york': 2, 'post': 3, 'angless': 4, 'loss': 5}\n",
            "\n",
            " new times\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPF_zM7lHbS5",
        "outputId": "36e3f810-064b-49e4-fb6f-09e9099aa834"
      },
      "source": [
        " #Vectorization :Bag Of word vector for each doc \n",
        " corpus_doc2bow_vectors=[dictionary.doc2bow(tok_doc) for tok_doc in tokenized_documents]\n",
        " #Print (Corpus_doc2bow_vectors) ,(id,tf)\n",
        " #For each document ,the words that appear (implicitly) zero times ,will not show up in the sparse \n",
        " #there will never be a element in the sparse vectors like (3,0)\n",
        "\n",
        "\n",
        " for c in corpus_doc2bow_vectors : \n",
        "   print(c)\n",
        "   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 1), (1, 1), (2, 1)]\n",
            "[(0, 1), (2, 1), (3, 1)]\n",
            "[(1, 1), (4, 1), (5, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3cmlRBvN5Be",
        "outputId": "7bb5b935-0fc2-40c8-95db-274de11c340d"
      },
      "source": [
        "# Train  (Compute ) TF_IDF \n",
        "# Note that TF_IDF Transformation only modifies feature weights of each words \n",
        "#it's input and output dimentionality are identical (=the dictionary size )\n",
        "#Computz the Tf_IDF by multiplying a local component (term frequency ) with a global component IDF \n",
        "#And normalizing the resulting documents to unit Length Formula .Formula For non-normilized weight of term \n",
        "#Of D Documents Weight {i,j} =Frequency {i,j} * log2 (D/document_Frequence_{i})\n",
        "\n",
        "%time tf_idf_model=models.TfidfModel(corpus_doc2bow_vectors,id2word=dictionary ,normalize=False)\n",
        "\n",
        "# apply model  \n",
        "corpus_tfidf_vectors=tf_idf_model[corpus_doc2bow_vectors]\n",
        "print(\"-------tfidf_vectors of the tree documents :[(id_word,tf_idf)]\")\n",
        "for doc_vect in corpus_tfidf_vectors : \n",
        "  print(doc_vect)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 140 µs, sys: 0 ns, total: 140 µs\n",
            "Wall time: 143 µs\n",
            "-------tfidf_vectors of the tree documents :[(id_word,tf_idf)]\n",
            "[(0, 0.5849625007211562), (1, 0.5849625007211562), (2, 0.5849625007211562)]\n",
            "[(0, 0.5849625007211562), (2, 0.5849625007211562), (3, 1.5849625007211563)]\n",
            "[(1, 0.5849625007211562), (4, 1.5849625007211563), (5, 1.5849625007211563)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOlrXGtSUwln",
        "outputId": "dfd8bf0f-7701-4ad8-e2c5-30be70a0bfc5"
      },
      "source": [
        "# Create a bow victors for a new documents  (for exemple a Query )\n",
        "#Human Compute \n",
        "query=\"new new times\"\n",
        "query_bow_vectors=dictionary.doc2bow(query.lower().split())\n",
        "print(query_bow_vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 2), (1, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYdg5Ky2Vei0",
        "outputId": "76f80439-6c90-4220-9853-b02c0b77c193"
      },
      "source": [
        "# Calculate the TFIDF VECTOR for the Query \n",
        "query_tfidf_vector=tf_idf_model[query_bow_vectors]\n",
        "print(query_tfidf_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 1.1699250014423124), (1, 0.5849625007211562)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2iWvhARV7b8",
        "outputId": "e73b44ab-6230-49c3-d209-b6f9e090f5f1"
      },
      "source": [
        "# Compute the cosine simularity between the Query and the three documents \n",
        "index_matrix=similarities.SparseMatrixSimilarity(corpus_tfidf_vectors,num_features=6)\n",
        "\n",
        "sims=index_matrix[query_tfidf_vector]\n",
        "print(list(enumerate(sims)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.7745967), (1, 0.2926428), (2, 0.112928025)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}